---
title: "Messenger Analysis"
format: html
---

```{python}
import json
import os
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib as plt
import gensim as gs
```

```{python}
import functions.fbmessenger as fbm
```

Import and clean data:
1. Convert to lowercase.
2. Letters and numbers only.
3. Remove more than 2 spaces.
4. Remove reactions.
5. Remove standard and custom stopwords.
6. Split string into list for word counts.

```{python}
chat_data = fbm.ms_import_data('data/the_office')

# clean messages
chat_data['clean_content'] = (
    chat_data['content']
    .str.lower()
    .str.strip()
    .str.replace('[^a-z0-9\\s]', '', regex=True)
    .str.replace('\\s{2,}', ' ', regex=True)
)

chat_data = chat_data[
    ~chat_data['clean_content']
    .str.contains('reacted to your message')
]

chat_data['split_content'] = (
    chat_data['clean_content']
    .str.split(' ')
)

custom_stopwords = [
    'u', 
    'lmao', 
    'lol', 
    'ur', 
    'like', 
    'yea', 
    'thats', 
    'nah', 
    'im', 
    'yeh', 
    'dont' 
    'yeah', 
    'gonna', 
    'didnt'
]

chat_data['clean_content'] = (
    chat_data['clean_content']
    .apply(gs.parsing.preprocessing.remove_stopwords)
    .apply(fbm.remove_custom_stopwords, args=(custom_stopwords,))
)
```

Count number of occurances of specified word by sender.

```{python}
search_word = 'nah'

chat_data_wcount = (
    chat_data[['sender_name', 'split_content']]
    .explode('split_content')
    .query('split_content == @search_word')
    .value_counts()
    .reset_index()
    .drop('split_content', axis=1)
    .sort_values('count', ascending=False)
)

chat_data_wcount
```

Split messages into conversations which are separated by at least 10 minutes.

```{python}
chat_data['time_diff'] = (
    chat_data['timestamp']
    .diff()
    .fillna(pd.Timedelta(seconds=0))
)
chat_data['time_diff'] = (
    chat_data['time_diff']
    .dt.total_seconds()
)

conv_cutoff = 600

chat_data['new_conv'] = chat_data['time_diff'] > conv_cutoff
chat_data['conv_num'] = 'Conv ' + (
    chat_data['new_conv']
    .cumsum()
    .astype(str)
)

conversations = (
    chat_data
    .groupby('conv_num')
    ['clean_content']
    .apply(lambda x: ' '.join(map(str, x)))
    .reset_index()
)
```

Run LDA, where documents are the previously defined conversations.

```{python}
documents = (
    conversations['clean_content']
    .tolist()
)
texts = [doc.split() for doc in documents]
dictionary = gs.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
ldamodel = gs.models.ldamodel.LdaModel(
    corpus,
    num_topics=10, 
    id2word=dictionary, 
    passes=20
)
for topic in ldamodel.print_topics(num_topics=10, num_words=5):
    print(topic)
```

```{python}
# group messages by sender
chat_data_grouped_messages = (
    chat_data
    .groupby('sender_name')
    ['content']
    .apply(lambda x: ' '.join(map(str, x)))
    .reset_index()
)
# get grouped string length
chat_data_grouped_messages['word_count'] = (
    chat_data_grouped_messages['content']
    .str.split()
    .str.len()
)
# sort by word count
chat_data_grouped_messages.sort_values('word_count', ascending = False)
```