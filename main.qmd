---
title: "Messenger Analysis"
format: html
---

```{python}
import json
import os
import time
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib as plt
import gensim as gs
```

```{python}
import functions.fbmessenger as fbm
```

Import and clean data:
1. Convert to lowercase.
2. Letters and numbers only.
3. Remove more than 2 spaces.
4. Remove reactions.
5. Remove standard and custom stopwords.
6. Split string into list for word counts.

```{python}
chat_data = fbm.ms_import_data('data/the_office')

# clean messages
chat_data['clean_content'] = (
    chat_data['content']
    .str.lower()
    .str.strip()
    .str.replace('[^a-z0-9\\s]', '', regex=True)
    .str.replace('\\s{2,}', ' ', regex=True)
)

chat_data = chat_data[
    ~chat_data['clean_content']
    .str.contains('reacted to your message')
]

chat_data['split_content'] = (
    chat_data['clean_content']
    .str.split(' ')
)

custom_stopwords = [
    'u', 
    'lmao', 
    'lol', 
    'ur', 
    'like', 
    'yea', 
    'thats', 
    'nah', 
    'im', 
    'yeh', 
    'dont' 
    'yeah', 
    'gonna', 
    'didnt'
]

chat_data['clean_content'] = (
    chat_data['clean_content']
    .apply(gs.parsing.preprocessing.remove_stopwords)
    .apply(fbm.remove_custom_stopwords, args=(custom_stopwords,))
)
```

Count number of occurances of specified word by sender.

```{python}
search_word = 'nah'

chat_data_wcount = (
    chat_data[['sender_name', 'split_content']]
    .explode('split_content')
    .query('split_content == @search_word')
    .value_counts()
    .reset_index()
    .drop('split_content', axis=1)
    .sort_values('count', ascending=False)
)

chat_data_wcount
```

Split messages into conversations which are separated by at least 10 minutes.

```{python}
chat_data['time_diff'] = (
    chat_data['timestamp']
    .diff()
    .fillna(pd.Timedelta(seconds=0))
)
chat_data['time_diff'] = (
    chat_data['time_diff']
    .dt.total_seconds()
)

conv_cutoff = 600

chat_data['new_conv'] = chat_data['time_diff'] > conv_cutoff
chat_data['conv_num'] = 'Conv ' + (
    chat_data['new_conv']
    .cumsum()
    .astype(str)
)

conversations = (
    chat_data
    .groupby('conv_num')
    ['clean_content']
    .apply(lambda x: ' '.join(map(str, x)))
    .reset_index()
)
```

Run LDA, where documents are the previously defined conversations.

```{python}
documents = (
    conversations['clean_content']
    .tolist()
)
texts = [doc.split() for doc in documents]
dictionary = gs.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
ldamodel = gs.models.ldamodel.LdaModel(
    corpus,
    num_topics=20,
    id2word=dictionary,
    passes=20
)
for topic in ldamodel.print_topics(num_topics=10, num_words=5):
    print(topic)
```

```{python}
ldamodel_coherence = gs.models.CoherenceModel(
    model=ldamodel,
    texts=texts,
    dictionary=dictionary,
    coherence='u_mass'
)

ldamodel_coherence.get_coherence()
```

Hyperparameter tuning

```{python}
alpha_values = np.arange(0.1, 1, 0.1)
beta_values = np.arange(0.1, 1, 0.1)
n_topics_values = np.arange(2, 21, 1)

tuning_parameters = (
    np.array(
        np.meshgrid(
            alpha_values,
            beta_values,
            n_topics_values
        )
    )
    .T
    .reshape(-1, 3)
)

tuning_results = pd.DataFrame(
    columns=[
        'Alpha',
        'Beta',
        'n_topics',
        'umass_coherence'
    ]
)

start_time = time.time()
for index in range(2):
    alpha = tuning_parameters[index][0]
    beta = tuning_parameters[index][1]
    n_topics = tuning_parameters[index][2]

    ldamodel = gs.models.ldamodel.LdaModel(
        corpus=corpus,
        alpha=alpha,
        eta=beta,
        num_topics=n_topics,
        id2word=dictionary,
        passes=20
    )

    ldamodel_coherence = gs.models.CoherenceModel(
        model=ldamodel,
        texts=texts,
        dictionary=dictionary,
        coherence='u_mass'
    )

    umass_coherence = ldamodel_coherence.get_coherence()

    tuning_results.loc[index] = (
        [alpha] + 
        [beta] + 
        [n_topics] + 
        [umass_coherence]
    )
end_time = time.time()
execution_time = end_time - start_time

tuning_results = tuning_results.sort_values('umass_coherence', ascending=False)

tuning_results
execution_time
```

```{python}
# group messages by sender
chat_data_grouped_messages = (
    chat_data
    .groupby('sender_name')
    ['content']
    .apply(lambda x: ' '.join(map(str, x)))
    .reset_index()
)
# get grouped string length
chat_data_grouped_messages['word_count'] = (
    chat_data_grouped_messages['content']
    .str.split()
    .str.len()
)
# sort by word count
chat_data_grouped_messages.sort_values('word_count', ascending = False)
```